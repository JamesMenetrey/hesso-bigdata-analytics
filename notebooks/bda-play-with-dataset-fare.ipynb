{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA Project\n",
    "- Romain Claret\n",
    "- Jämes Ménétrey\n",
    "- Damien Rochat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "#memory = '4g'\n",
    "#pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "#--driver-maxResultSize 10g --executor-memory 4g\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"bda-spark-fare\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '4g')\n",
    "        .set('spark.driver.memory', '4g')\n",
    "        .set('spark.driver.maxResultSize', '4g'))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '40061'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', 'rclaret.tic.heia-fr.ch'),\n",
       " ('spark.app.id', 'local-1559776876221'),\n",
       " ('spark.app.name', 'bda-spark-fare'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.maxResultSize', '4g')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Spark is working with a little PI calculation using monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13444\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_samples = 100000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fare_total: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def df_maker_fare(fare_path):\n",
    "    df_fare = (sqlContext.read.format(\"csv\")\n",
    "                .option(\"delimiter\", \",\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(fare_path)\n",
    "                .drop(\"medallion\")\n",
    "                .drop(\" hack_license\")\n",
    "                .drop(\"hack_license\")\n",
    "                .drop(\" vendor_id\")\n",
    "                .drop(\" pickup_datetime\")\n",
    "                .drop(\" payment_type\")\n",
    "                .drop(\" tip_amount\")\n",
    "                .drop(\" tolls_amount\")\n",
    "                .drop(\" total_amount\")\n",
    "               .drop(\" mta_tax\")\n",
    "               .withColumnRenamed(\" fare_amount\", \"fare_amount\")\n",
    "               .withColumnRenamed(\" surcharge\", \"surcharge\")\n",
    "              )\n",
    "    return (df_fare.withColumn(\"fare_total\",df_fare.fare_amount+df_fare.surcharge)\n",
    "               .drop(\"fare_amount\")\n",
    "               .drop(\"surcharge\")\n",
    "              )\n",
    "\n",
    "df_fare = df_maker_fare(\"datasets/trip_fare/trip_fare_1.csv\")\n",
    "\n",
    "df_fare.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ny_lat_min = 40.50214590272583\n",
    "ny_lat_max = 40.9#40.75977082462501\n",
    "ny_lon_min = -74.24354116993825\n",
    "ny_lon_max = -73.77490985242169\n",
    "    \n",
    "def df_maker_data(data_path):\n",
    "    df_data = (sqlContext.read.format(\"csv\")\n",
    "                .option(\"delimiter\", \",\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(data_path)\n",
    "                .drop(\"medallion\")\n",
    "                .drop(\" hack_license\")\n",
    "                .drop(\"hack_license\")\n",
    "                .drop(\" vendor_id\")\n",
    "                .drop(\"vendor_id\")\n",
    "               .drop(\" rate_code\")\n",
    "                .drop(\"rate_code\")\n",
    "               .drop(\" store_and_fwd_flag\")\n",
    "                .drop(\"store_and_fwd_flag\")\n",
    "               .drop(\" pickup_datetime\")\n",
    "                .drop(\"pickup_datetime\")\n",
    "               .drop(\" dropoff_datetime\")\n",
    "                .drop(\"dropoff_datetime\")\n",
    "               .drop(\" passenger_count\")\n",
    "                .drop(\"passenger_count\")\n",
    "               .drop(\" trip_time_in_secs\")\n",
    "                .drop(\"trip_time_in_secs\")\n",
    "               .drop(\" trip_distance\")\n",
    "                .drop(\"trip_distance\")\n",
    "               .drop(\" dropoff_longitude\")\n",
    "                .drop(\"dropoff_longitude\")\n",
    "               .drop(\" dropoff_latitude\")\n",
    "                .drop(\"dropoff_latitude\")\n",
    "               .withColumnRenamed(\" pickup_longitude\", \"pickup_longitude\")\n",
    "               .withColumnRenamed(\" pickup_latitude\", \"pickup_latitude\")\n",
    "                \n",
    "              )\n",
    "    return (df_data.withColumn('pickup_longitude', df_data['pickup_longitude'].cast('float')) #convert str to float\n",
    "            .withColumn('pickup_latitude', df_data['pickup_latitude'].cast('float'))\n",
    "            .filter(df_data.pickup_latitude<=ny_lat_max)\n",
    "            .filter(df_data.pickup_latitude>=ny_lat_min)\n",
    "            .filter(df_data.pickup_longitude<=ny_lon_max)\n",
    "            .filter(df_data.pickup_longitude>=ny_lon_min)\n",
    "           )\n",
    "\n",
    "df_data = df_maker_data(\"datasets/trip_data/trip_data_1.csv\")\n",
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |--  fare_amount: string (nullable = true)\n",
      " |--  surcharge: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_fare.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |--  vendor_id: string (nullable = true)\n",
      " |--  pickup_datetime: string (nullable = true)\n",
      " |--  payment_type: string (nullable = true)\n",
      " |--  fare_amount: string (nullable = true)\n",
      " |--  surcharge: string (nullable = true)\n",
      " |--  mta_tax: string (nullable = true)\n",
      " |--  tip_amount: string (nullable = true)\n",
      " |--  tolls_amount: string (nullable = true)\n",
      " |--  total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "def df_combiner(df_fare, df_data):\n",
    "    df_fare_id = df_fare.withColumn(\"id\", monotonically_increasing_id())\n",
    "    df_data_id = df_data.withColumn(\"id\", monotonically_increasing_id())\n",
    "    return df_fare_id.join(df_data_id, \"id\", \"outer\").drop(\"id\")\n",
    "#df_merged = df_combiner(df_fare, df_data)\n",
    "#df_merged.printSchema()\n",
    "#df_merged.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create complete Dataframe from all Dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+\n",
      "|fare_total|pickup_longitude|pickup_latitude|\n",
      "+----------+----------------+---------------+\n",
      "|       7.5|       -73.97865|      40.787735|\n",
      "|       7.5|       -73.97713|       40.74831|\n",
      "+----------+----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_trip = \"datasets/trip_data/\"\n",
    "path_fare = \"datasets/trip_fare/\"\n",
    "\n",
    "fare_files = os.listdir(path_fare)\n",
    "\n",
    "for idx,e in enumerate(os.listdir(path_trip)):\n",
    "    trip_file = path_trip+\"trip_data_\"+str(idx+1)+\".csv\"\n",
    "    fare_file = path_fare+\"trip_fare_\"+str(idx+1)+\".csv\"\n",
    "\n",
    "    if idx == 0:\n",
    "        dff = df_combiner(df_maker_fare(fare_file),\n",
    "                          df_maker_data(trip_file))\n",
    "    else:\n",
    "        df = df_combiner(df_maker_fare(fare_file),\n",
    "                          df_maker_data(trip_file))\n",
    "        dff= dff.union(df)\n",
    "\n",
    "dff.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173264090"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of the unique last dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13977692"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original 7774669\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with a smaller dataset: 1/9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using K-Means to show Clusters\n",
    "- pickup cluster\n",
    "- dropoff cluster\n",
    "\n",
    "### Cluster fitter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "def calculate_kmeans_cluster(df, model, feature_1, feature_2, cluser_name):\n",
    "    vecAssembler = VectorAssembler(inputCols=[feature_1, feature_2], outputCol=\"features\")\n",
    "    df_features = vecAssembler.transform(df)\n",
    "    \n",
    "    df = model.transform(df_features)\n",
    "    \n",
    "    df = df.withColumnRenamed('prediction', cluser_name)\n",
    "    df = df.drop('features')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means model building function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-mllib/spark-mllib-KMeans.html\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def build_kmeans_model(df=0,k_n=2,iter_n=20,steps_n=5,tol_n=1e-4,feature_1=\"\",feature_2=\"\"):\n",
    "    vecAssembler = VectorAssembler(inputCols=[feature_1, feature_2], outputCol=\"features\")\n",
    "    vec_df = vecAssembler.transform(df)\n",
    "    \n",
    "    kmeans = KMeans(k=k_n, seed=1, maxIter=iter_n, initSteps=steps_n, tol=tol_n)\n",
    "    \n",
    "    model = kmeans.fit(vec_df.select('features'))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
