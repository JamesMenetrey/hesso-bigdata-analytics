{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA Project\n",
    "- Romain Claret\n",
    "- Jämes Ménétrey\n",
    "- Damien Rochat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "#memory = '4g'\n",
    "#pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "#--driver-maxResultSize 10g --executor-memory 4g\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"bda-spark-fare\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '10g')\n",
    "        .set('spark.driver.memory', '10g')\n",
    "        .set('spark.driver.maxResultSize', '10g')\n",
    "        .set('spark.network.timeout', '1000000000')\n",
    "        .set('spark.executor.heartbeatInterval', '1000000000')\n",
    "        )\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '10g'),\n",
       " ('spark.driver.port', '40399'),\n",
       " ('spark.driver.memory', '10g'),\n",
       " ('spark.driver.host', 'rclaret.tic.heia-fr.ch'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'bda-spark-fare'),\n",
       " ('spark.driver.maxResultSize', '10g'),\n",
       " ('spark.network.timeout', '1000000000'),\n",
       " ('spark.executor.heartbeatInterval', '1000000000'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1559849514228'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Spark is working with a little PI calculation using monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14152\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_samples = 100000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_maker_fare(fare_path):\n",
    "    df_fare = (sqlContext.read.format(\"csv\")\n",
    "                .option(\"delimiter\", \",\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(fare_path)\n",
    "                .drop(\"medallion\")\n",
    "                .drop(\" hack_license\")\n",
    "                .drop(\"hack_license\")\n",
    "                .drop(\" vendor_id\")\n",
    "                .drop(\" pickup_datetime\")\n",
    "                .drop(\" payment_type\")\n",
    "                .drop(\" tip_amount\")\n",
    "                .drop(\" tolls_amount\")\n",
    "                .drop(\" total_amount\")\n",
    "               .drop(\" mta_tax\")\n",
    "               .withColumnRenamed(\" fare_amount\", \"fare_amount\")\n",
    "               .withColumnRenamed(\" surcharge\", \"surcharge\")\n",
    "              )\n",
    "    return (df_fare.withColumn(\"fare_total\",df_fare.fare_amount+df_fare.surcharge)\n",
    "               .drop(\"fare_amount\")\n",
    "               .drop(\"surcharge\")\n",
    "              )\n",
    "\n",
    "#df_fare = df_maker_fare(\"datasets/trip_fare/trip_fare_1.csv\")\n",
    "#\n",
    "#df_fare.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_maker_no_fare_filter(fare_path):\n",
    "    return (sqlContext.read.format(\"csv\")\n",
    "                .option(\"delimiter\", \",\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(fare_path)\n",
    "                .drop(\"medallion\")\n",
    "                .drop(\" hack_license\")\n",
    "                .drop(\"hack_license\")\n",
    "                .drop(\" vendor_id\")\n",
    "                .drop(\" pickup_datetime\")\n",
    "                .drop(\" payment_type\")\n",
    "                .drop(\" tip_amount\")\n",
    "                .drop(\" tolls_amount\")\n",
    "                .drop(\" total_amount\")\n",
    "               .drop(\" mta_tax\")\n",
    "               .withColumnRenamed(\" fare_amount\", \"fare_amount\")\n",
    "               .withColumnRenamed(\" surcharge\", \"surcharge\")\n",
    "              )\n",
    "\n",
    "#df_fare = df_maker_no_fare_filter(\"datasets/trip_fare/trip_fare_1.csv\")\n",
    "#\n",
    "#df_fare.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_fare_9.csv\n",
      "trip_fare_7.csv\n",
      "trip_fare_1.csv\n",
      "trip_fare_11.csv\n",
      "trip_fare_3.csv\n",
      "trip_fare_10.csv\n",
      "trip_fare_4.csv\n",
      "trip_fare_8.csv\n",
      "trip_fare_5.csv\n",
      "trip_fare_12.csv\n",
      "trip_fare_6.csv\n",
      "trip_fare_2.csv\n",
      "root\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      "\n",
      "+-----------+---------+\n",
      "|fare_amount|surcharge|\n",
      "+-----------+---------+\n",
      "|       11.5|        0|\n",
      "|       14.5|        0|\n",
      "+-----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_fare = \"datasets/trip_fare/\"\n",
    "for idx,e in enumerate(os.listdir(path_fare)):\n",
    "    print(e)\n",
    "    if idx == 0:\n",
    "        dff_no_fare_filter = df_maker_no_fare_filter(path_fare+e)\n",
    "    else:\n",
    "        df_no_fare_filter = df_maker_no_fare_filter(path_fare+e)\n",
    "        dff_no_fare_filter=dff_no_fare_filter.union(df_no_fare_filter)\n",
    "dff_no_fare_filter.printSchema()\n",
    "dff_no_fare_filter.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no fare filter partial: 13990176\n",
      "no fare filter full: 173179759\n"
     ]
    }
   ],
   "source": [
    "print(\"no fare filter partial:\",df_no_fare_filter.count())\n",
    "print(\"no fare filter full:\",dff_no_fare_filter.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_fare_filtered = dff_no_fare_filter.na.drop().filter(dff_no_fare_filter.fare_amount>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare filter full: 173171082\n"
     ]
    }
   ],
   "source": [
    "print(\"fare filter full:\",dff_fare_filtered.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"with fare filter partial:\",df_no_fare_filter.count())\n",
    "print(\"with fare filter full:\",dff_no_fare_filter.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_lat_min = 40.50214590272583\n",
    "ny_lat_max = 40.9#40.75977082462501\n",
    "ny_lon_min = -74.24354116993825\n",
    "ny_lon_max = -73.77490985242169\n",
    "    \n",
    "def df_maker_data(data_path):\n",
    "    df_data = (sqlContext.read.format(\"csv\")\n",
    "                .option(\"delimiter\", \",\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(data_path)\n",
    "                .drop(\"medallion\")\n",
    "                .drop(\" hack_license\")\n",
    "                .drop(\"hack_license\")\n",
    "                .drop(\" vendor_id\")\n",
    "                .drop(\"vendor_id\")\n",
    "               .drop(\" rate_code\")\n",
    "                .drop(\"rate_code\")\n",
    "               .drop(\" store_and_fwd_flag\")\n",
    "                .drop(\"store_and_fwd_flag\")\n",
    "               .drop(\" pickup_datetime\")\n",
    "                .drop(\"pickup_datetime\")\n",
    "               .drop(\" dropoff_datetime\")\n",
    "                .drop(\"dropoff_datetime\")\n",
    "               .drop(\" passenger_count\")\n",
    "                .drop(\"passenger_count\")\n",
    "               .drop(\" trip_time_in_secs\")\n",
    "                .drop(\"trip_time_in_secs\")\n",
    "               .drop(\" trip_distance\")\n",
    "                .drop(\"trip_distance\")\n",
    "               .drop(\" dropoff_longitude\")\n",
    "                .drop(\"dropoff_longitude\")\n",
    "               .drop(\" dropoff_latitude\")\n",
    "                .drop(\"dropoff_latitude\")\n",
    "               .withColumnRenamed(\" pickup_longitude\", \"pickup_longitude\")\n",
    "               .withColumnRenamed(\" pickup_latitude\", \"pickup_latitude\")\n",
    "                \n",
    "              )\n",
    "    return (df_data.withColumn('pickup_longitude', df_data['pickup_longitude'].cast('float')) #convert str to float\n",
    "            .withColumn('pickup_latitude', df_data['pickup_latitude'].cast('float'))\n",
    "            .filter(df_data.pickup_latitude<=ny_lat_max)\n",
    "            .filter(df_data.pickup_latitude>=ny_lat_min)\n",
    "            .filter(df_data.pickup_longitude<=ny_lon_max)\n",
    "            .filter(df_data.pickup_longitude>=ny_lon_min)\n",
    "           )\n",
    "\n",
    "#df_data = df_maker_data(\"datasets/trip_data/trip_data_1.csv\")\n",
    "#df_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_fare.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "def df_combiner(df_fare, df_data):\n",
    "    df_fare_id = df_fare.withColumn(\"id\", monotonically_increasing_id())\n",
    "    df_data_id = df_data.withColumn(\"id\", monotonically_increasing_id())\n",
    "    return df_fare_id.join(df_data_id, \"id\", \"outer\").drop(\"id\")\n",
    "#df_merged = df_combiner(df_fare, df_data)\n",
    "#df_merged.printSchema()\n",
    "#df_merged.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def distribute_cost(df):\n",
    "    columns = ['fare_total', 'pickup_longitude', 'pickup_latitude']\n",
    "    vals = [(1, -73.977776, 40.758053)]\n",
    "    df_tmp = sqlContext.createDataFrame(vals, columns)  \n",
    "    for r in df.toLocalIterator():\n",
    "        for i in range(ceil(r.fare_total)-1):\n",
    "            newRow = sqlContext.createDataFrame([(1,r.pickup_longitude,r.pickup_latitude)], columns)\n",
    "            df_tmp = df_tmp.union(newRow)\n",
    "    return df.union(df_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create complete Dataframe from all Dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: datasets/trip_data/trip_data_1.csv\n",
      "writes init parquet\n",
      "distributes cost\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ca85b01d34c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_backups\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"trip-dff-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distributes cost\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"writes distributed parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_backups\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"trip-dff-dist-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-8112150b8f16>\u001b[0m in \u001b[0;36mdistribute_cost\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfare_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mnewRow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickup_longitude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickup_latitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mdf_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewRow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1454\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \"\"\"\n\u001b[0;32m-> 1456\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_trip = \"datasets/trip_data/\"\n",
    "path_fare = \"datasets/trip_fare/\"\n",
    "path_backups = \"backups/\"\n",
    "columns = ['fare_total', 'pickup_longitude', 'pickup_latitude']\n",
    "\n",
    "fare_files = os.listdir(path_fare)\n",
    "\n",
    "for idx,e in enumerate(os.listdir(path_trip)):\n",
    "    trip_file = path_trip+\"trip_data_\"+str(idx+1)+\".csv\"\n",
    "    fare_file = path_fare+\"trip_fare_\"+str(idx+1)+\".csv\"\n",
    "\n",
    "    if idx == 0:\n",
    "        print(\"start: \" + trip_file)\n",
    "        dff = df_combiner(df_maker_fare(fare_file),\n",
    "                          df_maker_data(trip_file))\n",
    "        print(\"writes init parquet\")\n",
    "        dff.write.parquet(path_backups+\"trip-dff-\"+str(idx)+\".parquet\")\n",
    "        print(\"distributes cost\")\n",
    "        dff = distribute_cost(dff)\n",
    "        print(\"writes distributed parquet\")\n",
    "        dff.write.parquet(path_backups+\"trip-dff-dist-\"+str(idx)+\".parquet\")\n",
    "        print(\"end: \" + trip_file)\n",
    "    else:\n",
    "        print(\"start: \" + trip_file)\n",
    "        df = df_combiner(df_maker_fare(fare_file),\n",
    "                          df_maker_data(trip_file))\n",
    "        print(\"writes init parquet\")\n",
    "        df.write.parquet(path_backups+\"trip-df-\"+str(idx)+\".parquet\")\n",
    "        print(\"distributes cost\")\n",
    "        df = distribute_cost(df)\n",
    "        print(\"writes distributed parquet\")\n",
    "        df.write.parquet(path_backups+\"trip-df-dist-\"+str(idx)+\".parquet\")\n",
    "        print(\"unions\")\n",
    "        dff= dff.union(df)\n",
    "        print(\"writes master distributed parquet\")\n",
    "        dff.write.parquet(path_backups+\"trip-dff-dist-\"+str(idx)+\".parquet\")\n",
    "        print(\"end: \" + trip_file)\n",
    "\n",
    "#dff.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173264090"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original: 173264090\n",
    "dff.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of the unique last dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13977692"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original 7774669\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+\n",
      "|fare_total|pickup_longitude|pickup_latitude|\n",
      "+----------+----------------+---------------+\n",
      "|       6.5|      -73.977776|      40.758053|\n",
      "|      12.0|       -73.95221|       40.77743|\n",
      "+----------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.collect():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_small = df.limit(10)\n",
    "#df_small = df.head(10)\n",
    "#df_small.show()\n",
    "#type(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+\n",
      "|fare_total|pickup_longitude|pickup_latitude|\n",
      "+----------+----------------+---------------+\n",
      "|       6.5|      -73.977776|      40.758053|\n",
      "|      12.0|       -73.95221|       40.77743|\n",
      "|       1.0|      -73.977776|      40.758053|\n",
      "|       1.0|      -73.977776|      40.758053|\n",
      "|       1.0|      -73.977776|      40.758053|\n",
      "|       1.0|      -73.977776|      40.758053|\n",
      "|       1.0|      -73.977776|      40.758053|\n",
      "|       1.0|      -73.977776|      40.758053|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "|       1.0|       -73.95221|       40.77743|\n",
      "+----------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['fare_total', 'pickup_longitude', 'pickup_latitude']\n",
    "vals = [(6.5, -73.977776, 40.758053), (12.0, -73.95221, 40.77743)]\n",
    "df_tmp = sqlContext.createDataFrame(vals, columns)\n",
    "\n",
    "df_tmp = distribute_cost(df_tmp)\n",
    "df_tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"col1>col2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'fare_total[0]'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"fare_total\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.summary of DataFrame[fare_total: double, pickup_longitude: float, pickup_latitude: float]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with a smaller dataset: 1/9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribute cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['fare_total', 'pickup_longitude', 'pickup_latitude']\n",
    "\n",
    "vals = [(1, 2, 0), (2, 0, 1)]\n",
    "\n",
    "df = spark.createDataFrame(vals, columns)\n",
    "\n",
    "newRow = spark.createDataFrame([(4,5,7)], columns)\n",
    "appended = df.union(newRow)\n",
    "appended.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
