{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA Project\n",
    "- Romain Claret\n",
    "- Jämes Ménétrey\n",
    "- Damien Rochat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "#memory = '4g'\n",
    "#pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "#--driver-maxResultSize 10g --executor-memory 4g\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"bda-spark-fare-tmp\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '10g')\n",
    "        .set('spark.driver.memory', '10g')\n",
    "        .set('spark.driver.maxResultSize', '10g')\n",
    "        .set('spark.network.timeout', '1000000000')\n",
    "        .set('spark.executor.heartbeatInterval', '1000000000')\n",
    "        )\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '10g'),\n",
       " ('spark.driver.port', '33469'),\n",
       " ('spark.app.id', 'local-1559822973793'),\n",
       " ('spark.driver.memory', '10g'),\n",
       " ('spark.driver.host', 'rclaret.tic.heia-fr.ch'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.maxResultSize', '10g'),\n",
       " ('spark.app.name', 'bda-spark-fare-tmp'),\n",
       " ('spark.network.timeout', '1000000000'),\n",
       " ('spark.executor.heartbeatInterval', '1000000000'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Spark is working with a little PI calculation using monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14236\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_samples = 100000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_maker_fare(fare_path):\n",
    "    df_fare = (sqlContext.read.format(\"csv\")\n",
    "                .option(\"delimiter\", \",\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(fare_path)\n",
    "                .drop(\"medallion\")\n",
    "                .drop(\" hack_license\")\n",
    "                .drop(\"hack_license\")\n",
    "                .drop(\" vendor_id\")\n",
    "                .drop(\" pickup_datetime\")\n",
    "                .drop(\" payment_type\")\n",
    "                .drop(\" tip_amount\")\n",
    "                .drop(\" tolls_amount\")\n",
    "                .drop(\" total_amount\")\n",
    "               .drop(\" mta_tax\")\n",
    "               .withColumnRenamed(\" fare_amount\", \"fare_amount\")\n",
    "               .withColumnRenamed(\" surcharge\", \"surcharge\")\n",
    "              )\n",
    "    return (df_fare.withColumn(\"fare_total\",df_fare.fare_amount+df_fare.surcharge)\n",
    "               .drop(\"fare_amount\")\n",
    "               .drop(\"surcharge\")\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_lat_min = 40.50214590272583\n",
    "ny_lat_max = 40.9#40.75977082462501\n",
    "ny_lon_min = -74.24354116993825\n",
    "ny_lon_max = -73.77490985242169\n",
    "    \n",
    "def df_maker_data(data_path):\n",
    "    df_data = (sqlContext.read.format(\"csv\")\n",
    "                .option(\"delimiter\", \",\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(data_path)\n",
    "                .drop(\"medallion\")\n",
    "                .drop(\" hack_license\")\n",
    "                .drop(\"hack_license\")\n",
    "                .drop(\" vendor_id\")\n",
    "                .drop(\"vendor_id\")\n",
    "               .drop(\" rate_code\")\n",
    "                .drop(\"rate_code\")\n",
    "               .drop(\" store_and_fwd_flag\")\n",
    "                .drop(\"store_and_fwd_flag\")\n",
    "               .drop(\" pickup_datetime\")\n",
    "                .drop(\"pickup_datetime\")\n",
    "               .drop(\" dropoff_datetime\")\n",
    "                .drop(\"dropoff_datetime\")\n",
    "               .drop(\" passenger_count\")\n",
    "                .drop(\"passenger_count\")\n",
    "               .drop(\" trip_time_in_secs\")\n",
    "                .drop(\"trip_time_in_secs\")\n",
    "               .drop(\" trip_distance\")\n",
    "                .drop(\"trip_distance\")\n",
    "               .drop(\" dropoff_longitude\")\n",
    "                .drop(\"dropoff_longitude\")\n",
    "               .drop(\" dropoff_latitude\")\n",
    "                .drop(\"dropoff_latitude\")\n",
    "               .withColumnRenamed(\" pickup_longitude\", \"pickup_longitude\")\n",
    "               .withColumnRenamed(\" pickup_latitude\", \"pickup_latitude\")\n",
    "                \n",
    "              )\n",
    "    return (df_data.withColumn('pickup_longitude', df_data['pickup_longitude'].cast('float')) #convert str to float\n",
    "            .withColumn('pickup_latitude', df_data['pickup_latitude'].cast('float'))\n",
    "            .filter(df_data.pickup_latitude<=ny_lat_max)\n",
    "            .filter(df_data.pickup_latitude>=ny_lat_min)\n",
    "            .filter(df_data.pickup_longitude<=ny_lon_max)\n",
    "            .filter(df_data.pickup_longitude>=ny_lon_min)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "def df_combiner(df_fare, df_data):\n",
    "    df_fare_id = df_fare.withColumn(\"id\", monotonically_increasing_id())\n",
    "    df_data_id = df_data.withColumn(\"id\", monotonically_increasing_id())\n",
    "    return df_fare_id.join(df_data_id, \"id\", \"outer\").drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: datasets/trip_data/trip_data_2.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_3.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_4.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_5.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_6.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_7.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_8.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_9.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_10.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_11.csv\n",
      "toPandas\n",
      "to_csv\n",
      "start: datasets/trip_data/trip_data_12.csv\n",
      "toPandas\n",
      "to_csv\n"
     ]
    }
   ],
   "source": [
    "path_trip = \"datasets/trip_data/\"\n",
    "path_fare = \"datasets/trip_fare/\"\n",
    "path_combined = \"datasets/trip_curated/\"\n",
    "\n",
    "fare_files = os.listdir(path_fare)\n",
    "\n",
    "for idx,e in enumerate(os.listdir(path_trip)):\n",
    "    if idx!=0:\n",
    "        trip_file = path_trip+\"trip_data_\"+str(idx+1)+\".csv\"\n",
    "        fare_file = path_fare+\"trip_fare_\"+str(idx+1)+\".csv\"\n",
    "        curated_file = path_combined+\"trip_curated_\"+str(idx+1)+\".csv\"\n",
    "\n",
    "        print(\"start: \" + trip_file)\n",
    "        df_curated = df_combiner(df_maker_fare(fare_file), df_maker_data(trip_file))\n",
    "        print(\"toPandas\")\n",
    "        df_curated = df_curated.toPandas()\n",
    "        print(\"to_csv\")\n",
    "        df_curated.to_csv(curated_file)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
