{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA Project\n",
    "- Romain Claret\n",
    "- Jämes Ménétrey\n",
    "- Damien Rochat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "#memory = '4g'\n",
    "#pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "#--driver-maxResultSize 10g --executor-memory 4g\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"bda-spark-fare-clustering\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '10g')\n",
    "        .set('spark.driver.memory', '10g')\n",
    "        .set('spark.driver.maxResultSize', '10g')\n",
    "        .set('spark.network.timeout', '1000000000')\n",
    "        .set('spark.executor.heartbeatInterval', '1000000000')\n",
    "        )\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '10g'),\n",
       " ('spark.driver.memory', '10g'),\n",
       " ('spark.driver.host', 'rclaret.tic.heia-fr.ch'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1559845596378'),\n",
       " ('spark.app.name', 'bda-spark-fare-clustering'),\n",
       " ('spark.driver.maxResultSize', '10g'),\n",
       " ('spark.network.timeout', '1000000000'),\n",
       " ('spark.executor.heartbeatInterval', '1000000000'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '42617')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Spark is working with a little PI calculation using monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14624\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_samples = 100000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"datasets/trip_curated/distributed/trip_curated_1.csv_out\"\n",
    "df = sqlContext.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\", \"false\").load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "|_c0|      _c1|      _c2|\n",
      "+---+---------+---------+\n",
      "|7.5|-73.97865|40.787735|\n",
      "|7.5|-73.97865|40.787735|\n",
      "|7.5|-73.97865|40.787735|\n",
      "|7.5|-73.97865|40.787735|\n",
      "|7.5|-73.97865|40.787735|\n",
      "|7.5|-73.97865|40.787735|\n",
      "|7.5|-73.97865|40.787735|\n",
      "|7.5|-73.97865|40.787735|\n",
      "|7.5|-73.97713| 40.74831|\n",
      "|7.5|-73.97713| 40.74831|\n",
      "|7.5|-73.97713| 40.74831|\n",
      "|7.5|-73.97713| 40.74831|\n",
      "|7.5|-73.97713| 40.74831|\n",
      "|7.5|-73.97713| 40.74831|\n",
      "|7.5|-73.97713| 40.74831|\n",
      "|7.5|-73.97713| 40.74831|\n",
      "|6.5|-73.98308|40.762566|\n",
      "|6.5|-73.98308|40.762566|\n",
      "|6.5|-73.98308|40.762566|\n",
      "|6.5|-73.98308|40.762566|\n",
      "+---+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_maker(file_path):\n",
    "    return (sqlContext.read.format(\"csv\")\n",
    "            .option(\"delimiter\", \",\")\n",
    "            .option(\"header\", \"false\")\n",
    "            .load(file_path)\n",
    "            .drop(\"_c0\")\n",
    "            .withColumnRenamed(\"_c1\", \"longitude\")\n",
    "            .withColumnRenamed(\"_c2\", \"latitude\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      "\n",
      "+---------+---------+\n",
      "|longitude| latitude|\n",
      "+---------+---------+\n",
      "|-73.97865|40.787735|\n",
      "|-73.97865|40.787735|\n",
      "+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "125403354\n"
     ]
    }
   ],
   "source": [
    "#all: 180'743'398\n",
    "#max 20: 125'403'354\n",
    "file_path = \"datasets/trip_curated/distributed/trip_curated_1.csv_out\"\n",
    "df_part = df_maker(file_path)\n",
    "df_part.printSchema()\n",
    "df_part.show(2)\n",
    "print(df_part.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_lat_min = 40.50214590272583\n",
    "ny_lat_max = 40.9\n",
    "ny_lon_min = -74.24354116993825\n",
    "ny_lon_max = -73.77490985242169\n",
    "\n",
    "def df_filter(df):\n",
    "    return (df.withColumn('latitude', df['latitude'].cast('float'))\n",
    "            .withColumn('longitude', df['longitude'].cast('float'))\n",
    "            .filter(df.latitude<=ny_lat_max)\n",
    "            .filter(df.latitude>=ny_lat_min)\n",
    "            .filter(df.longitude<=ny_lon_max)\n",
    "            .filter(df.longitude>=ny_lon_min)\n",
    "            .na.drop()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      "\n",
      "+---------+---------+\n",
      "|longitude| latitude|\n",
      "+---------+---------+\n",
      "|-73.97865|40.787735|\n",
      "|-73.97865|40.787735|\n",
      "+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "123059346\n"
     ]
    }
   ],
   "source": [
    "#all: 177'388'377\n",
    "#max 20: 123'059'346\n",
    "df_part = df_filter(df_part)\n",
    "df_part.printSchema()\n",
    "df_part.show(2)\n",
    "print(df_part.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      "\n",
      "+---------+---------+\n",
      "|longitude| latitude|\n",
      "+---------+---------+\n",
      "|-73.97865|40.787735|\n",
      "|-73.97865|40.787735|\n",
      "+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "123059346\n"
     ]
    }
   ],
   "source": [
    "#all: 177'388'377\n",
    "#max 20: 123'059'346\n",
    "file_path = \"datasets/trip_curated/distributed/trip_curated_1.csv_out\"\n",
    "df_test = df_filter(df_maker(file_path))\n",
    "df_test.printSchema()\n",
    "df_test.show(2)\n",
    "print(df_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create complete Dataframe from all Dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_curated_3.csv_out\n",
      "trip_curated_4.csv_out\n",
      "trip_curated_8.csv_out\n",
      "trip_curated_2.csv_out\n",
      "trip_curated_7.csv_out\n",
      "trip_curated_9.csv_out\n",
      "trip_curated_12.csv_out\n",
      "trip_curated_5.csv_out\n",
      "trip_curated_1.csv_out\n",
      "trip_curated_6.csv_out\n",
      "trip_curated_11.csv_out\n",
      "trip_curated_10.csv_out\n",
      "+---------+---------+\n",
      "|longitude| latitude|\n",
      "+---------+---------+\n",
      "|-73.98638|40.749817|\n",
      "|-73.98638|40.749817|\n",
      "+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_trip = \"datasets/trip_curated/distributed_e3/\"\n",
    "for idx,e in enumerate(os.listdir(path_trip)):\n",
    "    print(e)\n",
    "    if idx == 0:\n",
    "        dff = df_filter(df_maker(path_trip+e))\n",
    "    else:\n",
    "        df = df_filter(df_maker(path_trip+e))\n",
    "        dff=dff.union(df)\n",
    "dff.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#none distributed: 85'147'405\n",
    "#distributed all: 2'192'938'097 # 7mins\n",
    "#distributed max 20: 1'461'307'161 # 5mins\n",
    "#distributed-e3 max 20: 268'806'940 # 50secs\n",
    "#distributed-e4 max 20: 351'520'721 # 1min\n",
    "#distributed-e5 max 20: 421'293'569 # 1min\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(dff.count())\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of the unique last dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36932762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'00:00:07'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not distributed: 4'834'280\n",
    "#distributed all: 195'909'011 # 1min\n",
    "#distributed max 20: 128'396'754 # 24secs\n",
    "#distributed-e3 max 20: 23'572'539 # 4secs\n",
    "#distributed-e4 max 20: 30'804'039 # 6secs\n",
    "#distributed-e5 max 20: 36'932'762 # 7secs\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(df.count())\n",
    "\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using K-Means to show Clusters\n",
    "- pickup cluster\n",
    "\n",
    "### Cluster fitter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "def calculate_kmeans_cluster(df, model, feature_1, feature_2, cluser_name):\n",
    "    vecAssembler = VectorAssembler(inputCols=[feature_1, feature_2], outputCol=\"features\")\n",
    "    df_features = vecAssembler.transform(df)\n",
    "    \n",
    "    df = model.transform(df_features)\n",
    "    \n",
    "    df = df.withColumnRenamed('prediction', cluser_name)\n",
    "    df = df.drop('features')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means model building function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-mllib/spark-mllib-KMeans.html\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def build_kmeans_model(df=0,k_n=2,iter_n=20,steps_n=5,tol_n=1e-4,feature_1=\"\",feature_2=\"\"):\n",
    "    vecAssembler = VectorAssembler(inputCols=[feature_1, feature_2], outputCol=\"features\")\n",
    "    vec_df = vecAssembler.transform(df)\n",
    "    \n",
    "    kmeans = KMeans(k=k_n, seed=1, maxIter=iter_n, initSteps=steps_n, tol=tol_n)\n",
    "    \n",
    "    model = kmeans.fit(vec_df.select('features'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization of the Custers from the small dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def kmeans_centroid_to_json(model, path):\n",
    "    print(\"Gets centroids\")\n",
    "    centers = model.clusterCenters()\n",
    "    print(\"To pd.df\")\n",
    "    centers_df = pd.DataFrame(centers)#, columns=[\"latitude\",\"longitude\"])\n",
    "    print(\"Filters\")\n",
    "    centers_df = centers_df[(centers_df[1] < ny_lon_max) &\n",
    "                     (centers_df[1] > ny_lon_min) &\n",
    "                     (centers_df[0] < ny_lat_max) &\n",
    "                     (centers_df[0] > ny_lat_min)\n",
    "                    ]\n",
    "    print(\"To Json\")\n",
    "    centers_df.to_json(path,orient='values')\n",
    "    print(centers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generates K Means Models Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builds KMeans with k = 5\n",
      "Calculates KMeans Clusters...\n",
      "KMeans Centroids to Json...\n",
      "Gets centroids\n",
      "To pd.df\n",
      "Filters\n",
      "To Json\n",
      "           0          1\n",
      "0  40.755531 -73.982636\n",
      "1  40.646847 -73.785576\n",
      "2  40.725290 -73.995518\n",
      "3  40.768084 -73.874792\n",
      "4  40.780146 -73.960625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done with: 00:58:24'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "def generate_centroids_kmeans_models(df, k_start, k_end, name):\n",
    "    for k_value in range(k_start,k_end,1):\n",
    "        df_copy = df\n",
    "        print(\"Builds KMeans with k =\",k_value)\n",
    "        df_model = build_kmeans_model(df_copy,k_n=k_value,iter_n=20,\n",
    "                                      steps_n=5,tol_n=1e-4,\n",
    "                                      feature_1=\"latitude\",\n",
    "                                      feature_2=\"longitude\")\n",
    "        time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time))\n",
    "        \n",
    "        print(\"Calculates KMeans Clusters...\")\n",
    "        df_copy = calculate_kmeans_cluster(df_copy, df_model, \"latitude\", \"longitude\", \"pickup_cluster\")\n",
    "        time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time))\n",
    "        \n",
    "        print(\"KMeans Centroids to Json...\")\n",
    "        kmeans_centroid_to_json(df_model,\"json/\"+name+str(k_value)+\".json\")\n",
    "\n",
    "start_time = time.time()\n",
    "generate_centroids_kmeans_models(dff, 5, 6, \"trip_fare_clusters_e3_k\")\n",
    "time.strftime(\"Done with: %H:%M:%S\", time.gmtime(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builds KMeans with k = 10\n",
      "Calculates KMeans Clusters...\n",
      "KMeans Centroids to Json...\n",
      "Gets centroids\n",
      "To pd.df\n",
      "Filters\n",
      "To Json\n",
      "           0          1\n",
      "0  40.747648 -73.994729\n",
      "1  40.646844 -73.785480\n",
      "2  40.756795 -73.975603\n",
      "3  40.729976 -73.984062\n",
      "4  40.720658 -74.004847\n",
      "5  40.769042 -73.871644\n",
      "6  40.800799 -73.959581\n",
      "7  40.772847 -73.954951\n",
      "8  40.685978 -73.979770\n",
      "9  40.775291 -73.981767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done with: 01:25:28'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "generate_centroids_kmeans_models(dff, 10, 11, \"trip_fare_clusters_e3_k\")\n",
    "time.strftime(\"Done with: %H:%M:%S\", time.gmtime(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builds KMeans with k = 15\n",
      "Calculates KMeans Clusters...\n",
      "KMeans Centroids to Json...\n",
      "Gets centroids\n",
      "To pd.df\n",
      "Filters\n",
      "To Json\n",
      "            0          1\n",
      "0   40.756422 -73.991603\n",
      "1   40.646829 -73.785461\n",
      "2   40.755930 -73.923114\n",
      "3   40.713731 -73.953623\n",
      "4   40.714500 -74.008769\n",
      "5   40.770124 -73.869128\n",
      "6   40.781603 -73.978466\n",
      "7   40.738943 -74.001724\n",
      "8   40.683958 -73.985319\n",
      "9   40.758963 -73.977791\n",
      "10  40.762862 -73.964856\n",
      "11  40.805768 -73.956833\n",
      "12  40.777538 -73.954485\n",
      "13  40.726240 -73.990271\n",
      "14  40.743062 -73.983651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done with: 01:18:05'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "generate_centroids_kmeans_models(dff, 15, 16, \"trip_fare_clusters_e3_k\")\n",
    "time.strftime(\"Done with: %H:%M:%S\", time.gmtime(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builds KMeans with k = 20\n",
      "Calculates KMeans Clusters...\n",
      "KMeans Centroids to Json...\n",
      "Gets centroids\n",
      "To pd.df\n",
      "Filters\n",
      "To Json\n",
      "            0          1\n",
      "0   40.760210 -73.969472\n",
      "1   40.646827 -73.785461\n",
      "2   40.748366 -73.991415\n",
      "3   40.710752 -74.011119\n",
      "4   40.759314 -73.989555\n",
      "5   40.769942 -73.869142\n",
      "6   40.769493 -73.982383\n",
      "7   40.683975 -73.985201\n",
      "8   40.804396 -73.958815\n",
      "9   40.737686 -73.985931\n",
      "10  40.713750 -73.953607\n",
      "11  40.832880 -73.939235\n",
      "12  40.781361 -73.951920\n",
      "13  40.725137 -74.001710\n",
      "14  40.770421 -73.959085\n",
      "15  40.740855 -74.002829\n",
      "16  40.787102 -73.974751\n",
      "17  40.724859 -73.988291\n",
      "18  40.750497 -73.977244\n",
      "19  40.755760 -73.923663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done with: 01:32:12'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "generate_centroids_kmeans_models(dff, 20, 21, \"trip_fare_clusters_e3_k\")\n",
    "time.strftime(\"Done with: %H:%M:%S\", time.gmtime(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builds KMeans with k = 7\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "generate_centroids_kmeans_models(dff, 7, 8, \"trip_fare_clusters_e3_k\")\n",
    "time.strftime(\"Done with: %H:%M:%S\", time.gmtime(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "generate_centroids_kmeans_models(dff, 12, 13, \"trip_fare_clusters_e3_k\")\n",
    "time.strftime(\"Done with: %H:%M:%S\", time.gmtime(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "generate_centroids_kmeans_models(dff, 17, 18, \"trip_fare_clusters_e3_k\")\n",
    "time.strftime(\"Done with: %H:%M:%S\", time.gmtime(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
